{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8136e9a9-4190-48e3-99c0-6becf45d0625",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis: Comparative Study of Naive Bayes and BERT\n",
    "\n",
    "## Introduction\n",
    "Financial sentiment analysis provides insights into market sentiments, helping investors and analysts make informed decisions. This project focuses on analyzing sentiments in financial reports using text classification methods, comparing the performance of traditional statistical model (Naive Bayes) and modern deep learning model (BERT models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b95e1-eaaf-41b5-b6e9-b44ea7c0a0d3",
   "metadata": {},
   "source": [
    "## Methodology:\n",
    "- The data was preprocessed using tokenization, lemmatization, and vectorization.\n",
    "- The Naive Bayes model was implemented using TF-IDF vectors, and the BERT model was fine-tuned on the financial sentiment dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c3dab-0339-4c8f-b681-5d21f4e7c7e1",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a3117c-ee4d-4057-9f64-fb2fe5842eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.39.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.5 MB 45.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 50.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.2/11.5 MB 51.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.8/11.5 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 31.2 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9441f27-6e5c-4e7a-996c-ad967c06f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting IProgress\n",
      "  Downloading IProgress-0.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting TensorFlow\n",
      "  Downloading tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.3.1-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: six in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from IProgress) (1.16.0)\n",
      "Collecting tensorflow-intel==2.17.0 (from TensorFlow)\n",
      "  Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading h5py-3.11.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading ml_dtypes-0.4.0-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->TensorFlow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->TensorFlow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->TensorFlow) (70.3.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->TensorFlow) (4.10.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading grpcio-1.64.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->TensorFlow) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.1-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading tbb-2021.13.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading optree-0.12.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.7/48.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->TensorFlow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->TensorFlow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->TensorFlow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->TensorFlow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->TensorFlow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->TensorFlow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Downloading tensorflow-2.17.0-cp312-cp312-win_amd64.whl (2.0 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
      "   ---------------------------------------- 0.0/385.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/385.2 MB 59.1 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 4.4/385.2 MB 56.5 MB/s eta 0:00:07\n",
      "    --------------------------------------- 7.0/385.2 MB 55.6 MB/s eta 0:00:07\n",
      "    --------------------------------------- 9.5/385.2 MB 55.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 12.0/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 14.5/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 17.1/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 19.5/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 22.1/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 24.6/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 27.1/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 29.7/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 32.2/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 34.6/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 37.2/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 39.7/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 42.2/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 44.6/385.2 MB 54.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 47.2/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 49.8/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 52.3/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 54.8/385.2 MB 54.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 57.3/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 59.9/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 62.4/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 64.9/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 67.5/385.2 MB 54.4 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 70.0/385.2 MB 54.4 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 72.6/385.2 MB 54.4 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 75.1/385.2 MB 50.1 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 77.7/385.2 MB 50.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 80.2/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 82.7/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 85.2/385.2 MB 50.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 87.8/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 90.3/385.2 MB 50.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 92.9/385.2 MB 59.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 95.5/385.2 MB 54.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 98.0/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 100.6/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 103.1/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 105.6/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 108.1/385.2 MB 50.4 MB/s eta 0:00:06\n",
      "   ----------- --------------------------- 110.7/385.2 MB 54.7 MB/s eta 0:00:06\n",
      "   ----------- --------------------------- 113.2/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 115.7/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 118.2/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------ -------------------------- 120.8/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------ -------------------------- 123.3/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------ -------------------------- 125.9/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 128.5/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 130.9/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 133.6/385.2 MB 59.5 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 136.0/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 138.5/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 141.0/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 143.6/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 146.1/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 148.7/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 151.2/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 153.7/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 156.2/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 158.7/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 161.3/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 163.8/385.2 MB 54.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 166.3/385.2 MB 54.7 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 168.9/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 171.4/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 174.0/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 176.6/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 179.1/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 181.7/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 184.2/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 186.6/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 189.2/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 191.7/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 194.3/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 196.8/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 199.4/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 202.0/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 204.5/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 207.1/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 209.6/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 212.1/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 214.7/385.2 MB 54.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 217.3/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 219.8/385.2 MB 54.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 222.4/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 224.9/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 227.4/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 229.9/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 232.5/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 235.0/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 237.5/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 240.0/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 242.6/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 245.2/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 247.7/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 250.3/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 252.7/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 255.2/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 257.7/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 260.3/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 262.9/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 265.5/385.2 MB 54.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 268.0/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 270.5/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 273.1/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 275.6/385.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 278.0/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 280.6/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 283.1/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 285.7/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 288.3/385.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 290.7/385.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 293.2/385.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 295.8/385.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 298.3/385.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 300.8/385.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 303.4/385.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 305.9/385.2 MB 50.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 308.4/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 311.0/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 313.5/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 316.1/385.2 MB 54.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 318.6/385.2 MB 59.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 321.2/385.2 MB 54.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 323.6/385.2 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 326.2/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 328.7/385.2 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 331.2/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 333.7/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 336.2/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 338.7/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 341.3/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 343.8/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 346.4/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 348.9/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 351.5/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 354.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 356.6/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 359.1/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 361.6/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 364.2/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.8/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 369.3/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 371.8/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 374.3/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.9/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  379.4/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.7/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.2/385.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 385.2/385.2 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.18.1-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.2/1.2 MB 77.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 37.8 MB/s eta 0:00:00\n",
      "Downloading torch-2.3.1-cp312-cp312-win_amd64.whl (159.7 MB)\n",
      "   ---------------------------------------- 0.0/159.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.7/159.7 MB 58.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 5.2/159.7 MB 55.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 7.8/159.7 MB 55.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 10.3/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 12.9/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 15.4/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 18.0/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 20.5/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 23.0/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 25.7/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 28.1/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 30.6/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 33.1/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 35.7/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 38.2/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 40.8/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 43.4/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 45.9/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 48.4/159.7 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 50.9/159.7 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 53.4/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 56.0/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 58.5/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 61.0/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 63.5/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 66.0/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 68.6/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 71.2/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 73.7/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 76.3/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 78.8/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 81.4/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 83.9/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 86.4/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 89.0/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 91.5/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 94.2/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 96.7/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 99.2/159.7 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 101.6/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 104.1/159.7 MB 54.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 106.6/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 109.0/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 111.6/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 114.1/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 116.6/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 119.2/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 121.7/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 124.3/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 126.8/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 129.4/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 132.0/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 134.6/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 137.1/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 139.6/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 142.2/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 144.7/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 147.3/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.9/159.7 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 152.3/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 154.9/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  157.4/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.7/159.7 MB 15.6 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.3.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.0/2.4 MB 65.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 37.9 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/228.5 MB 67.2 MB/s eta 0:00:04\n",
      "    --------------------------------------- 4.7/228.5 MB 59.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 7.3/228.5 MB 58.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 9.8/228.5 MB 56.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 12.4/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 14.9/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 17.4/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 19.9/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 22.3/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 24.8/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 27.3/228.5 MB 50.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 29.8/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 32.3/228.5 MB 50.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 34.8/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 37.2/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 39.8/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 42.3/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 44.9/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 47.4/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 50.0/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 52.5/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 55.0/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 57.5/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 60.1/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 62.6/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 65.1/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 67.6/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 70.2/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 72.7/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 75.3/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 77.9/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 80.4/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 82.9/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 85.4/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 88.0/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 90.5/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 93.1/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 95.7/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 98.2/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 100.6/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 103.2/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 105.8/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 108.2/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 110.7/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 113.2/228.5 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 115.7/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 118.2/228.5 MB 54.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 120.8/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 123.3/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 125.8/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 128.3/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 130.9/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 133.3/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 135.8/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 138.4/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 140.9/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 143.5/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 145.9/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 148.4/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 151.0/228.5 MB 50.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 153.4/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 156.0/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 158.5/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 161.1/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 163.7/228.5 MB 59.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 166.2/228.5 MB 59.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 168.7/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 171.3/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 173.8/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 176.3/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 178.8/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 181.4/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 183.9/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 186.4/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 188.9/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 191.4/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 194.0/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 196.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 199.0/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.6/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.2/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 206.6/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 209.2/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 211.6/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 214.2/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 216.6/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 219.1/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 221.7/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  224.1/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  226.6/228.5 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 14.9 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 2.5/3.5 MB 52.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 MB 56.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 32.2 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.13.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.9/286.9 kB ? eta 0:00:00\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.64.1-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 2.3/4.1 MB 48.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.1/4.1 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 32.5 MB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.8/3.0 MB 58.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 31.5 MB/s eta 0:00:00\n",
      "Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 23.9 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/26.4 MB 66.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.6/26.4 MB 58.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 7.1/26.4 MB 57.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.6/26.4 MB 55.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.1/26.4 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.7/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.2/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.3/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.8/26.4 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.0-cp312-cp312-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.5/127.5 kB ? eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 2.3/5.5 MB 48.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.5 MB 49.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 31.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.4/105.4 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 227.3/227.3 kB ? eta 0:00:00\n",
      "Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.8/65.8 kB ? eta 0:00:00\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp312-cp312-win_amd64.whl (267 kB)\n",
      "   ---------------------------------------- 0.0/267.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 267.2/267.2 kB 16.1 MB/s eta 0:00:00\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tbb, namex, libclang, intel-openmp, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mkl, mdurl, markdown, IProgress, h5py, grpcio, google-pasta, gast, absl-py, torch, tensorboard, markdown-it-py, astunparse, torchvision, torchaudio, rich, keras, tensorflow-intel, TensorFlow\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "Successfully installed IProgress-0.4 TensorFlow-2.17.0 absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 intel-openmp-2021.4.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 mkl-2021.4.0 ml-dtypes-0.4.0 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 protobuf-4.25.3 rich-13.7.1 tbb-2021.13.0 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-intel-2.17.0 termcolor-2.4.0 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1 werkzeug-3.0.3 wheel-0.43.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install IProgress TensorFlow torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad16f1d-9229-47b6-b176-dc1cffe5b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (70.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio setuptools \n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6896f1-0310-442d-96b4-ef91ca840e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (70.3.0)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kmusodza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fadd9a-36f9-4ce7-a755-096e8c5ef932",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f53ada-0360-4f1d-8898-83f6a3eec131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b383fea-ad06-4b32-bdea-a8be315866aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence sentiment\n",
      "0  According to Gran , the company has no plans t...   neutral\n",
      "1  For the last quarter of 2010 , Componenta 's n...  positive\n",
      "2  In the third quarter of 2010 , net sales incre...  positive\n",
      "3  Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
      "4  Operating profit totalled EUR 21.1 mn , up fro...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df_all_agree = pd.read_csv('FinancialPhraseBank-v1.0/Sentences_AllAgree.txt', sep='@', header=None, names=['sentence', 'sentiment'], encoding='ISO-8859-1')\n",
    "df_75_agree = pd.read_csv('FinancialPhraseBank-v1.0/Sentences_75Agree.txt', sep='@', header=None, names=['sentence', 'sentiment'], encoding='ISO-8859-1')\n",
    "df_66_agree = pd.read_csv('FinancialPhraseBank-v1.0/Sentences_66Agree.txt', sep='@', header=None, names=['sentence', 'sentiment'], encoding='ISO-8859-1')\n",
    "df_50_agree = pd.read_csv('FinancialPhraseBank-v1.0/Sentences_50Agree.txt', sep='@', header=None, names=['sentence', 'sentiment'], encoding='ISO-8859-1')\n",
    "\n",
    "# Choose one of the datasets for the project\n",
    "df = df_all_agree  # Using the dataset with 100% agreement\n",
    "# Display the first few rows to confirm it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e619a90-a98d-4472-8ddc-4d32e9763658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmusodza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\kmusodza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kmusodza\\.cache\\huggingface\\hub\\models--ahmedrachid--FinancialBERT-Sentiment-Analysis. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9998133778572083}, {'label': 'neutral', 'score': 0.9997822642326355}, {'label': 'negative', 'score': 0.9877365231513977}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\", num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales.\",\n",
    "    \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000.\",\n",
    "    \"Raute reported a loss per share of EUR 0.86 for the first half of 2009, against EPS of EUR 0.74 in the corresponding period of 2008.\"\n",
    "]\n",
    "\n",
    "# Analyze the sentiment\n",
    "results = nlp(sentences)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c41a9-5547-474d-9191-c80b4e60fcaf",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "We'll clean the data by removing any unnecessary characters and whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec54e21-6ccc-464b-8c8a-b07de26979f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>according to gran  the company has no plans to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for the last quarter of   componenta s net sal...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in the third quarter of   net sales increased ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>operating profit rose to eur  mn from eur  mn ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating profit totalled eur  mn  up from eur...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>operating result for the month period decrease...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>helsinki thomson financial  shares in cargotec...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>london marketwatch  share prices ended lower i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>operating profit fell to eur  mn from eur  mn ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>sales in finland decreased by   in january  wh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence sentiment\n",
       "0     according to gran  the company has no plans to...   neutral\n",
       "1     for the last quarter of   componenta s net sal...  positive\n",
       "2     in the third quarter of   net sales increased ...  positive\n",
       "3     operating profit rose to eur  mn from eur  mn ...  positive\n",
       "4     operating profit totalled eur  mn  up from eur...  positive\n",
       "...                                                 ...       ...\n",
       "2259  operating result for the month period decrease...  negative\n",
       "2260  helsinki thomson financial  shares in cargotec...  negative\n",
       "2261  london marketwatch  share prices ended lower i...  negative\n",
       "2262  operating profit fell to eur  mn from eur  mn ...  negative\n",
       "2263  sales in finland decreased by   in january  wh...  negative\n",
       "\n",
       "[2264 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(clean_text)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12778ba3-ce13-41b2-b2d2-922b82065d79",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We will tokenize the sentences using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b88c8e-0263-4a23-abb8-5368090a9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kmusodza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['tokens'] = df['sentence'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc127ea-a98d-4ed6-bd15-bd5d2c3eda6c",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatize the tokens to convert them to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9751d847-a4d6-4119-bbc4-a2f2f28fd72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kmusodza\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(lemmatize_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49c428-9388-4676-86a5-5d8ecabc6d7e",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Convert the text data into numerical format using TF-IDF for the Naive Bayes model and word embeddings for the BERT model.\n",
    "\n",
    "*TF-IDF Vectorization for Naive Bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31d6bb0-f217-498b-9f9d-33a8941a09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokens back to sentences for TF-IDF vectorization\n",
    "df['cleaned_sentence'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['cleaned_sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0729ee1-47ae-40fe-9d2b-8e5607987e10",
   "metadata": {},
   "source": [
    "#### Word Embeddings for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57338484-db9d-47a3-9a89-2f1dad84dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmusodza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kmusodza\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>bert_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>according to gran  the company has no plans to...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[according, to, gran, the, company, ha, no, pl...</td>\n",
       "      <td>according to gran the company ha no plan to mo...</td>\n",
       "      <td>[101, 2429, 2000, 12604, 1996, 2194, 2038, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for the last quarter of   componenta s net sal...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[for, the, last, quarter, of, componenta, s, n...</td>\n",
       "      <td>for the last quarter of componenta s net sale ...</td>\n",
       "      <td>[101, 2005, 1996, 2197, 4284, 1997, 6922, 2050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in the third quarter of   net sales increased ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[in, the, third, quarter, of, net, sale, incre...</td>\n",
       "      <td>in the third quarter of net sale increased by ...</td>\n",
       "      <td>[101, 1999, 1996, 2353, 4284, 1997, 5658, 4341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>operating profit rose to eur  mn from eur  mn ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[operating, profit, rose, to, eur, mn, from, e...</td>\n",
       "      <td>operating profit rose to eur mn from eur mn in...</td>\n",
       "      <td>[101, 4082, 5618, 3123, 2000, 7327, 2099, 2409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating profit totalled eur  mn  up from eur...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[operating, profit, totalled, eur, mn, up, fro...</td>\n",
       "      <td>operating profit totalled eur mn up from eur m...</td>\n",
       "      <td>[101, 4082, 5618, 2561, 3709, 7327, 2099, 2409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>finnish talentum reports its operating profit ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[finnish, talentum, report, it, operating, pro...</td>\n",
       "      <td>finnish talentum report it operating profit in...</td>\n",
       "      <td>[101, 6983, 5848, 2819, 4311, 2049, 4082, 5618...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clothing retail chain seppl s sales increased ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[clothing, retail, chain, seppl, s, sale, incr...</td>\n",
       "      <td>clothing retail chain seppl s sale increased b...</td>\n",
       "      <td>[101, 5929, 7027, 4677, 19802, 24759, 1055, 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>consolidated net sales increased   to reach eu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[consolidated, net, sale, increased, to, reach...</td>\n",
       "      <td>consolidated net sale increased to reach eur m...</td>\n",
       "      <td>[101, 10495, 5658, 4341, 3445, 2000, 3362, 732...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  \\\n",
       "0  according to gran  the company has no plans to...   neutral   \n",
       "1  for the last quarter of   componenta s net sal...  positive   \n",
       "2  in the third quarter of   net sales increased ...  positive   \n",
       "3  operating profit rose to eur  mn from eur  mn ...  positive   \n",
       "4  operating profit totalled eur  mn  up from eur...  positive   \n",
       "5  finnish talentum reports its operating profit ...  positive   \n",
       "6  clothing retail chain seppl s sales increased ...  positive   \n",
       "7  consolidated net sales increased   to reach eu...  positive   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [according, to, gran, the, company, ha, no, pl...   \n",
       "1  [for, the, last, quarter, of, componenta, s, n...   \n",
       "2  [in, the, third, quarter, of, net, sale, incre...   \n",
       "3  [operating, profit, rose, to, eur, mn, from, e...   \n",
       "4  [operating, profit, totalled, eur, mn, up, fro...   \n",
       "5  [finnish, talentum, report, it, operating, pro...   \n",
       "6  [clothing, retail, chain, seppl, s, sale, incr...   \n",
       "7  [consolidated, net, sale, increased, to, reach...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  according to gran the company ha no plan to mo...   \n",
       "1  for the last quarter of componenta s net sale ...   \n",
       "2  in the third quarter of net sale increased by ...   \n",
       "3  operating profit rose to eur mn from eur mn in...   \n",
       "4  operating profit totalled eur mn up from eur m...   \n",
       "5  finnish talentum report it operating profit in...   \n",
       "6  clothing retail chain seppl s sale increased b...   \n",
       "7  consolidated net sale increased to reach eur m...   \n",
       "\n",
       "                                         bert_tokens  \n",
       "0  [101, 2429, 2000, 12604, 1996, 2194, 2038, 205...  \n",
       "1  [101, 2005, 1996, 2197, 4284, 1997, 6922, 2050...  \n",
       "2  [101, 1999, 1996, 2353, 4284, 1997, 5658, 4341...  \n",
       "3  [101, 4082, 5618, 3123, 2000, 7327, 2099, 2409...  \n",
       "4  [101, 4082, 5618, 2561, 3709, 7327, 2099, 2409...  \n",
       "5  [101, 6983, 5848, 2819, 4311, 2049, 4082, 5618...  \n",
       "6  [101, 5929, 7027, 4677, 19802, 24759, 1055, 43...  \n",
       "7  [101, 10495, 5658, 4341, 3445, 2000, 3362, 732...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize sentences for BERT\n",
    "df['bert_tokens'] = df['sentence'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4c7f9-8c6b-4382-9c8a-cf2bff1cc2a0",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "We'll implement and evaluate two models: a Naive Bayes classifier using TF-IDF vectors and a BERT model for sentiment analysis.\n",
    "\n",
    "### Implement the Naive Bayes Classifier\n",
    "\n",
    "- Split data\n",
    "- Train data classifier\n",
    "- Predict on test set\n",
    "- Evaluate classsifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf2643b-2764-4821-af77-8f6b00aaf010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "Accuracy: 0.7858719646799117\n",
      "Precision: 0.8327235772357723\n",
      "Recall: 0.5789893571514125\n",
      "F1-Score: 0.5767244438177758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Map sentiments to numerical labels\n",
    "sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "df['sentiment_label'] = df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment_label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the Naive Bayes classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Naive Bayes Classifier\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1809160-5288-440b-8bb5-4f13efa8e96a",
   "metadata": {},
   "source": [
    "### BERT Model for Sentiment Analysis\n",
    "#### Implement the BERT Model\n",
    "\n",
    "We'll use the transformers library to fine-tune a pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f81723-149c-45d1-9095-7682f2caf756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\kmusodza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Train loss 0.36738332534176216 accuracy 0.8586416344561016\n",
      "Val loss 0.1509924430785508 accuracy 0.9492273730684326\n",
      "Epoch 2/4\n",
      "Train loss 0.09417342842278774 accuracy 0.9745996686913307\n",
      "Val loss 0.14169721671476446 accuracy 0.9580573951434879\n",
      "Epoch 3/4\n",
      "Train loss 0.044061490187519474 accuracy 0.9895085588072888\n",
      "Val loss 0.1489185376959885 accuracy 0.9602649006622517\n",
      "Epoch 4/4\n",
      "Train loss 0.02463073427421286 accuracy 0.992821645499724\n",
      "Val loss 0.15294025441760134 accuracy 0.9558498896247241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset class for BERT\n",
    "class FinancialSentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'sentence_text': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['sentiment'])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(df['sentence'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = FinancialSentimentDataset(train_sentences.values, train_labels.values, tokenizer, MAX_LEN)\n",
    "test_dataset = FinancialSentimentDataset(test_sentences.values, test_labels.values, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * 4  # 4 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / len(dataloader.dataset), total_loss / len(dataloader)\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model = model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return correct_predictions.double() / len(dataloader.dataset), total_loss / len(dataloader)\n",
    "\n",
    "# Train the BERT model\n",
    "EPOCHS = 4\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    val_acc, val_loss = eval_model(model, test_dataloader, device)\n",
    "    print(f\"Val loss {val_loss} accuracy {val_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda0bdd-dcc7-45e1-ab98-0edbdfff5c82",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a804710-5612-425b-a627-cf4107a24864",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Evaluation\n",
    "- Use Confusion matrix to viusally evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3752fe8-c022-4e29-b956-07a8ad480fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Evaluation\n",
      "Accuracy: 0.7859\n",
      "Precision: 0.8327\n",
      "Recall: 0.5790\n",
      "F1-Score: 0.5767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.09      0.16        56\n",
      "           0       0.82      0.98      0.89       276\n",
      "           1       0.68      0.67      0.67       121\n",
      "\n",
      "    accuracy                           0.79       453\n",
      "   macro avg       0.83      0.58      0.58       453\n",
      "weighted avg       0.81      0.79      0.74       453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the Naive Bayes classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Naive Bayes Classifier Evaluation\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abcc7eb8-17cd-4010-80cd-8ebb55eee20c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Naive Bayes Classifier Evaluation\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "nb_cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633fa28-5b9b-4177-9f60-eefe08c9cc4c",
   "metadata": {},
   "source": [
    "### BERT Model Evaluation\n",
    "\n",
    "- We'll use the evaluation function defined earlier to calculate the metrics for the BERT model.\n",
    "- Use Confusion matrix to viusally evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bec0c3-f191-4b11-9407-2df959742bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1, true_labels, predictions\n",
    "\n",
    "# Evaluate the BERT model\n",
    "bert_accuracy, bert_precision, bert_recall, bert_f1, bert_true, bert_pred = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "print(\"BERT Model Evaluation\")\n",
    "print(f\"Accuracy: {bert_accuracy:.4f}\")\n",
    "print(f\"Precision: {bert_precision:.4f}\")\n",
    "print(f\"Recall: {bert_recall:.4f}\")\n",
    "print(f\"F1-Score: {bert_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(bert_true, bert_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d5aff-3a3b-481e-9338-929d4021f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the BERT model on the test set\n",
    "bert_accuracy, bert_precision, bert_recall, bert_f1, bert_true, bert_pred = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "print(\"BERT Model Evaluation\")\n",
    "print(f\"Accuracy: {bert_accuracy:.4f}\")\n",
    "print(f\"Precision: {bert_precision:.4f}\")\n",
    "print(f\"Recall: {bert_recall:.4f}\")\n",
    "print(f\"F1-Score: {bert_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(bert_true, bert_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "bert_cm = confusion_matrix(bert_true, bert_pred)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(bert_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.title('Confusion Matrix - BERT')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986f710-ae5a-42d1-8a8a-a474c7952b2c",
   "metadata": {},
   "source": [
    "### Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222cfc9-8e5d-4ac7-9b89-28f90488ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save Naive Bayes model\n",
    "joblib.dump(nb_classifier, 'naive_bayes_model.pkl')\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save the trained BERT model and tokenizer\n",
    "model.save_pretrained('bert_model')\n",
    "tokenizer.save_pretrained('bert_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90108043-b54f-4c23-9f1f-8d24a39425c8",
   "metadata": {},
   "source": [
    "## Comparison and Analysis of Naive Bayes and BERT Models\n",
    "| Metric          | Naive Bayes | BERT      |\n",
    "|-----------------|-------------|-----------|\n",
    "| Accuracy        | 0.7859      | 0.9669    |\n",
    "| Precision       | 0.8327      | 0.9538    |\n",
    "| Recall          | 0.5790      | 0.9553    |\n",
    "| F1-Score        | 0.5767      | 0.9545    |\n",
    "\n",
    "\n",
    "Classification Reports\n",
    "\n",
    "Naive Bayes Classifier:\n",
    "\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "          -1       1.00      0.09      0.16        56\n",
    "           0       0.82      0.98      0.89       276\n",
    "           1       0.68      0.67      0.67       121\n",
    "\n",
    "    accuracy                           0.79       453\n",
    "   macro avg       0.83      0.58      0.58       453\n",
    "weighted avg       0.81      0.79      0.74       453\n",
    "\n",
    "\n",
    "BERT Model:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.95      0.94        56\n",
    "           1       0.98      0.99      0.98       276\n",
    "           2       0.95      0.93      0.94       121\n",
    "\n",
    "    accuracy                           0.97       453\n",
    "   macro avg       0.95      0.96      0.95       453\n",
    "weighted avg       0.97      0.97      0.97       453\n",
    "\n",
    "\n",
    "### **Analysis**\n",
    "\n",
    "- Naive Bayes: Simple and fast, but struggles with negative sentiments.\n",
    "\n",
    "- BERT: Highly accurate and robust, capable of understanding complex language patterns and context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ea677-ce5f-4bcc-8016-48cd7fcabcb0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### Naive Bayes Classifier:\n",
    "\n",
    "**Strengths**:\n",
    "\n",
    "- Simplicity and speed: The Naive Bayes classifier is easy to implement and computationally efficient.\n",
    "- High precision for neutral sentiments (0.82) and acceptable accuracy (0.7859).\n",
    "\n",
    "**Weaknesses**:\n",
    "\n",
    "- Poor recall for negative sentiments (0.09), indicating that the model struggles to identify negative instances.\n",
    "- Lower overall F1-Score (0.5767), especially for negative sentiments, suggesting that the model doesn't perform well across all classes.\n",
    "\n",
    "\n",
    "### Bert model\n",
    "**Strengths**:\n",
    "\n",
    "- High accuracy (0.9669), indicating that the model is very good at predicting the correct sentiment.\n",
    "- Balanced precision (0.9538) and recall (0.9553), showing that the model can reliably identify all sentiment classes.\n",
    "- High F1-Score (0.9545) across all classes, demonstrating the model's robustness and effectiveness in financial sentiment analysis.\n",
    "\n",
    "**Weaknesses**:\n",
    "\n",
    "- Computationally intensive: Training and inference with BERT require significant resources.\n",
    "- Complexity: Implementing and fine-tuning BERT is more complex compared to simpler models like Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d9d69-076a-4f39-8d78-8bfbb3487902",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "- The BERT model is highly effective for financial sentiment analysis, providing accurate and reliable predictions.\n",
    "- The Naive Bayes classifier, although simpler, is less effective in this context but could be useful for quick, preliminary analysis.\n",
    "- Future work could explore further fine-tuning of BERT, experimenting with other advanced models, or combining multiple models for even better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
